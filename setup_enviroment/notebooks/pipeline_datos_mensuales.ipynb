{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ccfab6-3f96-47dd-801b-1b611e8b5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e266b3fe-1243-496d-8705-d22fa140dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed659a-7161-4478-88c0-3df7a541b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffcbbe56-cc87-4ec9-adf1-d0bfcf7ee005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 05:13:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Batch Log Processing with Iceberg\") \\\n",
    "    .config(\"spark.ui.port\", \"4041\") \\\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \",\".join([\n",
    "            \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.5.jar\",\n",
    "            \"/opt/spark/jars/hadoop-aws-3.2.2.jar\",\n",
    "            \"/opt/spark/jars/aws-java-sdk-1.11.375.jar\",\n",
    "            \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.8.1.jar\",\n",
    "            \"/opt/spark/jars/postgresql-42.5.0.jar\",\n",
    "            \"/opt/spark/jars/commons-pool2-2.11.1.jar\"\n",
    "        ])\n",
    "    ) \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"s3a://warehouse/logs\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.io\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.s3.access.key.id\", \"admin\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.s3.secret.access.key\", \"password\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.s3.region\", \"us-east-1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3965e1d4-90c0-4823-998e-06c0423a9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc048cd-f5b6-4219-8524-5a1f4a4cdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customers_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"customers_dataset\") \\\n",
    "    .load()\n",
    "\n",
    "geolocation_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"geolocation_dataset\") \\\n",
    "    .load()\n",
    "\n",
    "order_items_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"order_items_dataset\") \\\n",
    "    .load()\n",
    "\n",
    "order_payments_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"order_payments_dataset\") \\\n",
    "    .load()\n",
    "\n",
    "order_reviews_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"order_reviews_dataset\") \\\n",
    "    .load()\n",
    "\n",
    "orders_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"orders_dataset\") \\\n",
    "    .load()\n",
    "\n",
    "products_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"products_dataset\") \\\n",
    "    .load()\n",
    "\n",
    "sellers_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"sellers_dataset\") \\\n",
    "    .load()\n",
    "\n",
    "product_category_name_translation_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"product_category_name_translation\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c26e01-07d9-47ad-9284-2aef95286c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertir value a STRING\n",
    "#logs_df = logs_df.selectExpr(\"CAST(value AS STRING) as log_line\")\n",
    "customers_df = customers_df.selectExpr(\"CAST(value AS STRING) as customers_line\")\n",
    "geolocation_df = geolocation_df.selectExpr(\"CAST(value AS STRING) as geolocation_line\")\n",
    "order_items_df = order_items_df.selectExpr(\"CAST(value AS STRING) as order_items_line\")\n",
    "order_payments_df = order_payments_df.selectExpr(\"CAST(value AS STRING) as order_payments_line\")\n",
    "order_reviews_df = order_reviews_df.selectExpr(\"CAST(value AS STRING) as order_reviews_line\")\n",
    "orders_df = orders_df.selectExpr(\"CAST(value AS STRING) as orders_line\")\n",
    "products_df = products_df.selectExpr(\"CAST(value AS STRING) as products_line\")\n",
    "sellers_df = sellers_df.selectExpr(\"CAST(value AS STRING) as sellers_line\")\n",
    "product_category_name_translation_df = product_category_name_translation_df.selectExpr(\"CAST(value AS STRING) as product_category_name_translation_line\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7168e3e8-be77-4524-8876-1055710e2c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# castear el string de filebeat de customers\n",
    "if len(customers_df.head(1)) > 0:\n",
    "    json_rdd = customers_df.select(\"customers_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"customer_id\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"customer_unique_id\", col(\"message_fields\").getItem(1)) \\\n",
    "                       .withColumn(\"customer_zip_code_prefix\", col(\"message_fields\").getItem(2)) \\\n",
    "                       .withColumn(\"customer_city\", col(\"message_fields\").getItem(3)) \\\n",
    "                       .withColumn(\"customer_state\", col(\"message_fields\").getItem(4))\n",
    "    #clean_df = clean_df.withColumn(\"customer_zip_code_prefix\", col(\"customer_zip_code_prefix\").cast(\"int\"))\n",
    "    #clean_df = clean_df.withColumn(\n",
    "    #    \"customer_zip_code_prefix\",\n",
    "    #    regexp_replace(col(\"customer_zip_code_prefix\"), '\"', '').cast(\"int\")\n",
    "    #)\n",
    "    clean_df1 = clean_df[['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5390d1d3-aee1-470d-b0a7-66e7b5f4bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# castear el string de filebeat de geolocation_line\n",
    "if len(geolocation_df.head(1)) > 0:\n",
    "    \n",
    "    json_rdd = geolocation_df.select(\"geolocation_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"geolocation_zip_code_prefix\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"geolocation_latitude\", col(\"message_fields\").getItem(1)) \\\n",
    "                       .withColumn(\"geolocation_longitude\", col(\"message_fields\").getItem(2)) \\\n",
    "                       .withColumn(\"geolocation_city\", col(\"message_fields\").getItem(3)) \\\n",
    "                       .withColumn(\"geolocation_state\", col(\"message_fields\").getItem(4))\n",
    "    \n",
    "    clean_df2 = clean_df[['geolocation_zip_code_prefix','geolocation_latitude','geolocation_longitude','geolocation_city','geolocation_state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bb45863-9a08-4ef9-9d53-5a30d8b82ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# castear el string de filebeat de order_items_line\n",
    "if len(order_items_df.head(1)) > 0:\n",
    "    \n",
    "    json_rdd = order_items_df.select(\"order_items_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"order_id\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"order_item_id\", col(\"message_fields\").getItem(1)) \\\n",
    "                       .withColumn(\"product_id\", col(\"message_fields\").getItem(2)) \\\n",
    "                       .withColumn(\"seller_id\", col(\"message_fields\").getItem(3)) \\\n",
    "                       .withColumn(\"shipping_limit_date\", col(\"message_fields\").getItem(4)) \\\n",
    "                       .withColumn(\"price\", col(\"message_fields\").getItem(5)) \\\n",
    "                       .withColumn(\"freight_value\", col(\"message_fields\").getItem(6))\n",
    "    \n",
    "    clean_df3 = clean_df[['order_id','order_item_id','product_id','seller_id','shipping_limit_date','price','freight_value']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b53ea2-3569-4a6d-8c70-1d3ae0212996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# castear el string de filebeat de order_payments_line\n",
    "if len(order_payments_df.head(1)) > 0:\n",
    "    \n",
    "    json_rdd = order_payments_df.select(\"order_payments_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"order_id\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"payment_sequential\", col(\"message_fields\").getItem(1)) \\\n",
    "                       .withColumn(\"payment_type\", col(\"message_fields\").getItem(2)) \\\n",
    "                       .withColumn(\"payment_installments\", col(\"message_fields\").getItem(3)) \\\n",
    "                       .withColumn(\"payment_value\", col(\"message_fields\").getItem(4))\n",
    "    \n",
    "    clean_df4 = clean_df[['order_id','payment_sequential','payment_type','payment_installments','payment_value']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97d96d5-6668-4393-8bd6-c157667c9b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# castear el string de filebeat de order_reviews_line\n",
    "if len(order_reviews_df.head(1)) > 0:\n",
    "    \n",
    "    json_rdd = order_reviews_df.select(\"order_reviews_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"review_id\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"order_id\", col(\"message_fields\").getItem(1)) \\\n",
    "                       .withColumn(\"review_score\", col(\"message_fields\").getItem(2)) \\\n",
    "                       .withColumn(\"review_comment_title\", col(\"message_fields\").getItem(3)) \\\n",
    "                       .withColumn(\"review_comment_message\", col(\"message_fields\").getItem(4)) \\\n",
    "                       .withColumn(\"review_creation_date\", col(\"message_fields\").getItem(5)) \\\n",
    "                       .withColumn(\"review_answer_timestamp\", col(\"message_fields\").getItem(6))\n",
    "    \n",
    "    clean_df5 = clean_df[['review_id','order_id','review_score','review_comment_title','review_comment_message','review_creation_date','review_answer_timestamp']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c38469-cb59-4e58-804a-486ee5333b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# castear el string de filebeat de orders_line\n",
    "if len(orders_df.head(1)) > 0:\n",
    "    \n",
    "    json_rdd = orders_df.select(\"orders_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"order_id\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"customer_id\", col(\"message_fields\").getItem(1)) \\\n",
    "                       .withColumn(\"order_status\", col(\"message_fields\").getItem(2)) \\\n",
    "                       .withColumn(\"order_purchase_timestamp\", col(\"message_fields\").getItem(3)) \\\n",
    "                       .withColumn(\"order_approved_at\", col(\"message_fields\").getItem(4)) \\\n",
    "                       .withColumn(\"order_delivered_carrier_date\", col(\"message_fields\").getItem(5)) \\\n",
    "                       .withColumn(\"order_delivered_customer_date\", col(\"message_fields\").getItem(6)) \\\n",
    "                       .withColumn(\"order_estimated_delivery_date\", col(\"message_fields\").getItem(6))\n",
    "    \n",
    "    clean_df6 = clean_df[['order_id','customer_id','order_status','order_purchase_timestamp','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0871713a-4bec-43a6-a43a-a8a4b5f1e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# castear el string de filebeat de products_line\n",
    "if len(products_df.head(1)) > 0:\n",
    "    \n",
    "    json_rdd = products_df.select(\"products_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"product_id\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"product_category_name\", col(\"message_fields\").getItem(1)) \\\n",
    "                       .withColumn(\"product_name_lenght\", col(\"message_fields\").getItem(2)) \\\n",
    "                       .withColumn(\"product_description_lenght\", col(\"message_fields\").getItem(3)) \\\n",
    "                       .withColumn(\"product_photos_qty\", col(\"message_fields\").getItem(4)) \\\n",
    "                       .withColumn(\"product_weight_g\", col(\"message_fields\").getItem(5)) \\\n",
    "                       .withColumn(\"product_length_cm\", col(\"message_fields\").getItem(6)) \\\n",
    "                       .withColumn(\"product_height_cm\", col(\"message_fields\").getItem(7)) \\\n",
    "                       .withColumn(\"product_width_cm\", col(\"message_fields\").getItem(8))\n",
    "    \n",
    "    clean_df7 = clean_df[['product_id','product_category_name','product_name_lenght','product_description_lenght','product_photos_qty','product_weight_g','product_length_cm','product_height_cm','product_width_cm']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "047095e2-04a1-45b7-9e68-ec6f5aa347e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# castear el string de filebeat de sellers_line\n",
    "if len(sellers_df.head(1)) > 0:\n",
    "    \n",
    "    json_rdd = sellers_df.select(\"sellers_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"seller_id\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"seller_zip_code_prefix\", col(\"message_fields\").getItem(1)) \\\n",
    "                       .withColumn(\"seller_city\", col(\"message_fields\").getItem(2)) \\\n",
    "                       .withColumn(\"seller_state\", col(\"message_fields\").getItem(3)) \n",
    "    \n",
    "    clean_df8 = clean_df[['seller_id','seller_zip_code_prefix','seller_city','seller_state']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "839658ab-90f9-4c29-b7fe-1f8bcfc5cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# castear el string de filebeat de product_category_name_translation_line\n",
    "if len(product_category_name_translation_df.head(1)) > 0:\n",
    "    \n",
    "    json_rdd = product_category_name_translation_df.select(\"product_category_name_translation_line\").rdd.map(lambda r: r[0])\n",
    "    json_df = spark.read.json(json_rdd)\n",
    "    \n",
    "    clean_df = json_df.withColumn(\"message_clean\", regexp_replace(col(\"message\"), '\\\\\"', '\"'))\n",
    "    clean_df = clean_df.withColumn(\"message_fields\", split(col(\"message_clean\"), \",\"))\n",
    "    \n",
    "    clean_df = clean_df.withColumn(\"product_category_name\", col(\"message_fields\").getItem(0)) \\\n",
    "                       .withColumn(\"product_category_name_english\", col(\"message_fields\").getItem(1))\n",
    "    \n",
    "    clean_df9 = clean_df[['product_category_name','product_category_name_english']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7ad46db-7481-44cb-b351-d452a48c6c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.logs\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.customers\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.geolocation\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.order_items\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.order_payments\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.order_reviews\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.orders\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.products\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.sellers\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.product_category_name_translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bdcfa2-7c00-4a8a-8f90-4725bfe277dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.logs (\n",
    "    log_line STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/logs'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.customers (\n",
    "    customers_line STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/customers'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c58e6d72-bd34-43bb-894d-9c4f6039366d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.customers (\n",
    "    customer_id STRING,\n",
    "    customer_unique_id STRING,\n",
    "    customer_zip_code_prefix STRING,\n",
    "    customer_city STRING,\n",
    "    customer_state STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/customers'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7677e6e2-5d77-449f-822d-f646814f4534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.geolocation (\n",
    "    geolocation_zip_code_prefix STRING,\n",
    "    geolocation_latitude STRING,\n",
    "    geolocation_longitude STRING,\n",
    "    geolocation_city STRING,\n",
    "    geolocation_state STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/geolocation'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f2255ce-5952-48a2-8541-fe670903c83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.order_items (\n",
    "    order_id STRING,\n",
    "    order_item_id STRING,\n",
    "    product_id STRING,\n",
    "    seller_id STRING,\n",
    "    shipping_limit_date STRING,\n",
    "    price STRING,\n",
    "    freight_value STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/order_items'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eee2bb2-b908-41f1-a676-e8964f8387a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.order_payments (\n",
    "    order_id STRING,\n",
    "    payment_sequential STRING,\n",
    "    payment_type STRING,\n",
    "    payment_installments STRING,\n",
    "    payment_value STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/order_payments'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b00f9c9-d173-46fe-9af0-c9c32b479e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.order_reviews (\n",
    "    review_id STRING,\n",
    "    order_id STRING,\n",
    "    review_score STRING,\n",
    "    review_comment_title STRING,\n",
    "    review_comment_message STRING,\n",
    "    review_creation_date STRING,\n",
    "    review_answer_timestamp STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/order_reviews'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1450ed4b-4fd3-4eb5-8de7-d46ddef9c7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.orders (\n",
    "    order_id STRING,\n",
    "    customer_id STRING,\n",
    "    order_status STRING,\n",
    "    order_purchase_timestamp STRING,\n",
    "    order_approved_at STRING,\n",
    "    order_delivered_carrier_date STRING,\n",
    "    order_delivered_customer_date STRING,\n",
    "    order_estimated_delivery_date STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/orders'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "293451f8-530f-46ef-ba09-9b6cf33e582c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.products (\n",
    "    product_id STRING,\n",
    "    product_category_name STRING,\n",
    "    product_name_lenght STRING,\n",
    "    product_description_lenght STRING,\n",
    "    product_photos_qty STRING,\n",
    "    product_weight_g STRING,\n",
    "    product_length_cm STRING,\n",
    "    product_height_cm STRING,\n",
    "    product_width_cm STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/products'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8173dfc5-148c-44bc-9271-77b2272dc390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.sellers (\n",
    "    seller_id STRING,\n",
    "    seller_zip_code_prefix STRING,\n",
    "    seller_city STRING,\n",
    "    seller_state STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/sellers'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2c4c98a-74bd-4ed9-bbde-f7afad09c3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.product_category_name_translation (\n",
    "    product_category_name STRING,\n",
    "    product_category_name_english STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION 's3a://warehouse/logs/default/product_category_name_translation'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb2697fb-57d7-466b-95a0-b182e72866be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "clean_df1.writeTo(\"hadoop_catalog.default.customers\").append()\n",
    "clean_df2.writeTo(\"hadoop_catalog.default.geolocation\").append()\n",
    "clean_df3.writeTo(\"hadoop_catalog.default.order_items\").append()\n",
    "clean_df4.writeTo(\"hadoop_catalog.default.order_payments\").append()\n",
    "clean_df5.writeTo(\"hadoop_catalog.default.order_reviews\").append()\n",
    "clean_df6.writeTo(\"hadoop_catalog.default.orders\").append()\n",
    "clean_df7.writeTo(\"hadoop_catalog.default.products\").append()\n",
    "clean_df8.writeTo(\"hadoop_catalog.default.sellers\").append()\n",
    "clean_df9.writeTo(\"hadoop_catalog.default.product_category_name_translation\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69700b-2107-4a4d-b241-7207b03b7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "no\n",
    "# Si prefieres escribir en PostgreSQL, usa este código\n",
    "logs_parsed.write \\\n",
    " .format(\"jdbc\") \\\n",
    " .option(\"url\", \"jdbc:postgresql://postgres:5432/iceberg\") \\\n",
    " .option(\"dbtable\", \"logs\") \\\n",
    " .option(\"user\", \"iceberg\") \\\n",
    " .option(\"password\", \"password\") \\\n",
    " .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    " .mode(\"append\") \\\n",
    " .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72615043-4c66-448c-9511-52260945dc50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+--------------------------+---------------------+----------------+\n",
      "|customer_id                       |customer_unique_id                |customer_zip_code_prefix  |customer_city        |customer_state  |\n",
      "+----------------------------------+----------------------------------+--------------------------+---------------------+----------------+\n",
      "|b2d1536598b73a9abd18e0d75d92f0a3  |\"918dc87cd72cd9f6ed4bd442ed785235\"|\"18682\"                   |lencois paulista     |SP              |\n",
      "|\"customer_id\"                     |\"customer_unique_id\"              |\"customer_zip_code_prefix\"|\"customer_city\"      |\"customer_state\"|\n",
      "|\"06b8999e2fba1a1fbc88172c00ba8bc7\"|\"861eff4711a542e4b93843c6dd7febb0\"|\"14409\"                   |franca               |SP              |\n",
      "|\"18955e83d337fd6b2def6b18a428ac77\"|\"290c77bc529b7ac935b93aa66c333dc3\"|\"09790\"                   |sao bernardo do campo|SP              |\n",
      "|\"4e7b3e00288586ebd08712fdd0374a03\"|\"060e732b5b29e8181a18229c7b0b2b5e\"|\"01151\"                   |sao paulo            |SP              |\n",
      "+----------------------------------+----------------------------------+--------------------------+---------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.customers LIMIT 5\")\n",
    "df_test.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc697441-9fa8-4e00-8c7e-bf6596648ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+--------------------+---------------------+------------------+-------------------+\n",
      "|geolocation_zip_code_prefix  |geolocation_latitude|geolocation_longitude|geolocation_city  |geolocation_state  |\n",
      "+-----------------------------+--------------------+---------------------+------------------+-------------------+\n",
      "|\"01035\"                      |-23.541577961711493 |-46.64160722329613   |sao paulo         |SP                 |\n",
      "|\"01012\"                      |-23.547762303364266 |-46.63536053788448   |são paulo         |SP                 |\n",
      "|\"01047\"                      |-23.546273112412678 |-46.64122516971552   |sao paulo         |SP                 |\n",
      "|\"01013\"                      |-23.546923208436723 |-46.6342636964915    |sao paulo         |SP                 |\n",
      "|\"geolocation_zip_code_prefix\"|\"geolocation_lat\"   |\"geolocation_lng\"    |\"geolocation_city\"|\"geolocation_state\"|\n",
      "+-----------------------------+--------------------+---------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.geolocation LIMIT 5\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "120f58c2-d2c5-42fc-ad19-8d83846eff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------------+----------------------------------+----------------------------------+---------------------+-------+---------------+\n",
      "|order_id                          |order_item_id  |product_id                        |seller_id                         |shipping_limit_date  |price  |freight_value  |\n",
      "+----------------------------------+---------------+----------------------------------+----------------------------------+---------------------+-------+---------------+\n",
      "|\"00048cc3ae777c65dbb7d2a0634bc1ea\"|1              |ef92defde845ab8450f9d70c526ef70f  |\"6426d21aca402a131fc0a5d0960a3c90\"|2017-05-23 03:55:27  |21.90  |12.69          |\n",
      "|\"00054e8431b9d7675808bcb819fb4a32\"|1              |\"8d4f2bb7e93e6710a28f34fa83ee7d28\"|\"7040e82f899a04d1b434b795a43b4617\"|2017-12-14 12:10:31  |19.90  |11.85          |\n",
      "|\"000576fe39319847cbb9d288c5617fa6\"|1              |\"557d850972a7d6f792fd18ae1400d9b6\"|\"5996cddab893a4652a15592fb58ab8db\"|2018-07-10 12:30:45  |810.00 |70.75          |\n",
      "|\"order_id\"                        |\"order_item_id\"|\"product_id\"                      |\"seller_id\"                       |\"shipping_limit_date\"|\"price\"|\"freight_value\"|\n",
      "|\"00010242fe8c5a6d1ba2dd792cb16214\"|1              |\"4244733e06e7ecb4970a6e2683c13e61\"|\"48436dade18ac8b2bce089ec2a041202\"|2017-09-19 09:45:35  |58.90  |13.29          |\n",
      "+----------------------------------+---------------+----------------------------------+----------------------------------+---------------------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.order_items LIMIT 5\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a40015af-bcac-4b06-8f20-07b71628590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------------+------------+--------------------+-------------+\n",
      "|order_id                          |payment_sequential|payment_type|payment_installments|payment_value|\n",
      "+----------------------------------+------------------+------------+--------------------+-------------+\n",
      "|ba78997921bbcdc1373bb41e913ab953  |1                 |credit_card |8                   |107.78       |\n",
      "|\"42fdf880ba16b47b59251dd489d4441a\"|1                 |credit_card |2                   |128.45       |\n",
      "|\"298fcdf1f73eb413e4d26d01b25bc1cd\"|1                 |credit_card |2                   |96.12        |\n",
      "|\"771ee386b001f06208a7419e4fc1bbd7\"|1                 |credit_card |1                   |81.16        |\n",
      "|\"3d7239c394a212faae122962df514ac7\"|1                 |credit_card |3                   |51.84        |\n",
      "+----------------------------------+------------------+------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.order_payments LIMIT 5\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18a62039-858c-4dda-bc25-63b1d04653fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+------------+--------------------+----------------------------------------------------------------------------------------------------+--------------------+-----------------------+\n",
      "|review_id                         |order_id                          |review_score|review_comment_title|review_comment_message                                                                              |review_creation_date|review_answer_timestamp|\n",
      "+----------------------------------+----------------------------------+------------+--------------------+----------------------------------------------------------------------------------------------------+--------------------+-----------------------+\n",
      "|\"e64fb393e7b32834bb789ff8bb30750e\"|\"658677c97b385a9be170737859d3511b\"|5           |                    |Recebi bem antes do prazo estipulado.                                                               |2017-04-21 00:00:00 |2017-04-21 22:02:06    |\n",
      "|\"f7c4243c7fe1938f181bec41a392bdeb\"|\"8e6bfb81e283fa7e4f11123a3fb894f1\"|5           |                    |Parabéns lojas lannister adorei comprar pela Internet seguro e prático Parabéns a todos feliz Páscoa|2018-03-01 00:00:00 |2018-03-02 10:26:53    |\n",
      "|\"15197aa66ff4d0650b5434f1b46cda19\"|\"b18dcdf73be66366873cd26c5724d1dc\"|1           |                    |                                                                                                    |2018-04-13 00:00:00 |2018-04-16 00:39:37    |\n",
      "|\"07f9bee5d1b850860defd761afa7ff16\"|\"e48aa0d2dcec3a2e87348811bcfdf22b\"|5           |                    |                                                                                                    |2017-07-16 00:00:00 |2017-07-18 19:30:34    |\n",
      "|\"7c6400515c67679fbee952a7525281ef\"|\"c31a859e34e3adac22f376954e19b39d\"|5           |                    |                                                                                                    |2018-08-14 00:00:00 |2018-08-14 21:36:06    |\n",
      "+----------------------------------+----------------------------------+------------+--------------------+----------------------------------------------------------------------------------------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.order_reviews LIMIT 5\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0767bfe3-9eaf-4dab-b961-11d840148e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+--------------+--------------------------+-------------------+------------------------------+-------------------------------+-------------------------------+\n",
      "|order_id                          |customer_id                       |order_status  |order_purchase_timestamp  |order_approved_at  |order_delivered_carrier_date  |order_delivered_customer_date  |order_estimated_delivery_date  |\n",
      "+----------------------------------+----------------------------------+--------------+--------------------------+-------------------+------------------------------+-------------------------------+-------------------------------+\n",
      "|ad21c59c0840e6cb83a9ceb5573f8159  |\"8ab97904e6daea8866dbdbc4fb7aad2c\"|delivered     |2018-02-13 21:18:39       |2018-02-13 22:20:29|2018-02-14 19:46:34           |2018-02-16 18:17:02            |2018-02-16 18:17:02            |\n",
      "|a4591c265e18cb1dcee52889e2d8acc3  |\"503740e9ca751ccdda7ba28e9ab8f608\"|delivered     |2017-07-09 21:57:05       |2017-07-09 22:10:13|2017-07-11 14:58:04           |2017-07-26 10:57:55            |2017-07-26 10:57:55            |\n",
      "|\"136cce7faa42fdb2cefd53fdc79a6098\"|ed0271e0b7da060a393796590e7b737a  |invoiced      |2017-04-11 12:22:08       |2017-04-13 13:25:17|                              |                               |                               |\n",
      "|\"6514b8ad8028c9f2cc2374ded245783f\"|\"9bdf08b4b3b52b5526ff42d37d47f222\"|delivered     |2017-05-16 13:10:30       |2017-05-16 13:22:11|2017-05-22 10:07:46           |2017-05-26 12:55:51            |2017-05-26 12:55:51            |\n",
      "|\"order_id\"                        |\"customer_id\"                     |\"order_status\"|\"order_purchase_timestamp\"|\"order_approved_at\"|\"order_delivered_carrier_date\"|\"order_delivered_customer_date\"|\"order_delivered_customer_date\"|\n",
      "+----------------------------------+----------------------------------+--------------+--------------------------+-------------------+------------------------------+-------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.orders LIMIT 5\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38fc60cf-577f-4751-bee3-353e7a1d8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----------------------+---------------------+----------------------------+--------------------+------------------+-------------------+-------------------+------------------+\n",
      "|product_id                        |product_category_name  |product_name_lenght  |product_description_lenght  |product_photos_qty  |product_weight_g  |product_length_cm  |product_height_cm  |product_width_cm  |\n",
      "+----------------------------------+-----------------------+---------------------+----------------------------+--------------------+------------------+-------------------+-------------------+------------------+\n",
      "|\"9dc1a7de274444849c219cff195d0b71\"|utilidades_domesticas  |37                   |402                         |4                   |625               |20                 |17                 |13                |\n",
      "|\"41d3672d4792049fa1779bb35283ed13\"|instrumentos_musicais  |60                   |745                         |1                   |200               |38                 |5                  |11                |\n",
      "|\"732bd381ad09e530fe0a5f457d81becb\"|cool_stuff             |56                   |1272                        |4                   |18350             |70                 |24                 |44                |\n",
      "|\"2548af3e6e77a690cf3eb6368e9ab61e\"|moveis_decoracao       |56                   |184                         |2                   |900               |40                 |8                  |40                |\n",
      "|\"product_id\"                      |\"product_category_name\"|\"product_name_lenght\"|\"product_description_lenght\"|\"product_photos_qty\"|\"product_weight_g\"|\"product_length_cm\"|\"product_height_cm\"|\"product_width_cm\"|\n",
      "+----------------------------------+-----------------------+---------------------+----------------------------+--------------------+------------------+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.products LIMIT 5\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d883e3d-79bd-4a93-9ccd-a81297ebd046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------+-----------------+------------+\n",
      "|seller_id                         |seller_zip_code_prefix|seller_city      |seller_state|\n",
      "+----------------------------------+----------------------+-----------------+------------+\n",
      "|c0f3eea2e14555b6faeea3dd58c1b1c3  |\"04195\"               |sao paulo        |SP          |\n",
      "|\"51a04a8a6bdcb23deccc82b0b80742cf\"|\"12914\"               |braganca paulista|SP          |\n",
      "|c240c4061717ac1806ae6ee72be3533b  |\"20920\"               |rio de janeiro   |RJ          |\n",
      "|e49c26c3edfa46d227d5121a6b6e4d37  |\"55325\"               |brejao           |PE          |\n",
      "|\"1b938a7ec6ac5061a66a3766e0e75f90\"|\"16304\"               |penapolis        |SP          |\n",
      "+----------------------------------+----------------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.sellers LIMIT 5\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812f7f49-982a-482b-adde-fafaef4d9dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 05:14:01 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/11/19 05:14:01 WARN FileSystem: Failed to initialize fileystem s3a://warehouse/logs: java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36.sql.\n: org.apache.iceberg.exceptions.RuntimeIOException: Failed to get file system for path: s3a://warehouse/logs\n\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:58)\n\tat org.apache.iceberg.hadoop.HadoopCatalog.initialize(HadoopCatalog.java:112)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1297)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1608)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1587)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:631)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:597)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:268)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:56)\n\t... 99 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClasses(Configuration.java:2663)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:628)\n\t... 108 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_test \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM hadoop_catalog.default.product_category_name_translation LIMIT 5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_test\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.sql.\n: org.apache.iceberg.exceptions.RuntimeIOException: Failed to get file system for path: s3a://warehouse/logs\n\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:58)\n\tat org.apache.iceberg.hadoop.HadoopCatalog.initialize(HadoopCatalog.java:112)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1297)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1608)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1587)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:631)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:597)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:268)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:56)\n\t... 99 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClasses(Configuration.java:2663)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:628)\n\t... 108 more\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM hadoop_catalog.default.product_category_name_translation LIMIT 5\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1d764-c7ef-4ed7-9424-ad1db8696e86",
   "metadata": {},
   "source": [
    "### otros ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ef127c-c2de-4edb-8da6-4b54bcb6e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 05:14:24 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/11/19 05:14:24 WARN FileSystem: Failed to initialize fileystem s3a://warehouse/logs: java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36.sql.\n: org.apache.iceberg.exceptions.RuntimeIOException: Failed to get file system for path: s3a://warehouse/logs\n\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:58)\n\tat org.apache.iceberg.hadoop.HadoopCatalog.initialize(HadoopCatalog.java:112)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1297)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:631)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:597)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:268)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:56)\n\t... 81 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClasses(Configuration.java:2663)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:628)\n\t... 90 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_count \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT count(*) FROM hadoop_catalog.default.customers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_count\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.sql.\n: org.apache.iceberg.exceptions.RuntimeIOException: Failed to get file system for path: s3a://warehouse/logs\n\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:58)\n\tat org.apache.iceberg.hadoop.HadoopCatalog.initialize(HadoopCatalog.java:112)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1297)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:631)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:597)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:268)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:56)\n\t... 81 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClasses(Configuration.java:2663)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:628)\n\t... 90 more\n"
     ]
    }
   ],
   "source": [
    "df_count = spark.sql(\"SELECT count(*) FROM hadoop_catalog.default.customers\")\n",
    "df_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3028790f-bb0a-43d7-9528-ad3396152f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|0       |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count = spark.sql(\"SELECT count(*) FROM hadoop_catalog.default.geolocation\")\n",
    "df_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4fccc92-8f73-4769-9c4d-01bf2fe54bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|  default|  customers|      false|\n",
      "|  default|geolocation|      false|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN hadoop_catalog.default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0293acb7-10c6-49d2-b4c7-c3739933cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+----------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|  customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------------+----------------+\n",
      "|\"879864dab9bc3047...|\"4c93744516667ad3...|                 \"89254\"|      jaragua do sul|              SC|\n",
      "|fd826e7cf63160e53...|addec96d2e059c80c...|                 \"04534\"|           sao paulo|              SP|\n",
      "|\"5e274e7a0c3809e1...|\"57b2a98a409812fe...|                 \"35182\"|             timoteo|              MG|\n",
      "|       \"customer_id\"|\"customer_unique_id\"|    \"customer_zip_cod...|     \"customer_city\"|\"customer_state\"|\n",
      "|\"4b7139f34592b3a3...|\"9afe194fb833f79e...|                 \"30575\"|      belo horizonte|              MG|\n",
      "|\"9fb35e4ed6f0a14a...|\"2a7745e1ed516b28...|                 \"39400\"|       montes claros|              MG|\n",
      "|\"5aa9e4fdd4dfd209...|\"2a46fb94aef5cbee...|                 \"20231\"|      rio de janeiro|              RJ|\n",
      "|b2d1536598b73a9ab...|\"918dc87cd72cd9f6...|                 \"18682\"|    lencois paulista|              SP|\n",
      "|       \"customer_id\"|\"customer_unique_id\"|    \"customer_zip_cod...|     \"customer_city\"|\"customer_state\"|\n",
      "|\"06b8999e2fba1a1f...|\"861eff4711a542e4...|                 \"14409\"|              franca|              SP|\n",
      "|\"18955e83d337fd6b...|\"290c77bc529b7ac9...|                 \"09790\"|sao bernardo do c...|              SP|\n",
      "|\"4e7b3e00288586eb...|\"060e732b5b29e818...|                 \"01151\"|           sao paulo|              SP|\n",
      "|b2b6027bc5c5109e5...|\"259dac757896d24d...|                 \"08775\"|     mogi das cruzes|              SP|\n",
      "|\"4f2d8ab171c80ec8...|\"345ecd01c38d18a9...|                 \"13056\"|            campinas|              SP|\n",
      "+--------------------+--------------------+------------------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('hadoop_catalog.default.customers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d32baf8e-e5bd-4dd1-8a6e-7bbe7850606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+---------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|geolocation_latitude|geolocation_longitude|geolocation_city|geolocation_state|\n",
      "+---------------------------+--------------------+---------------------+----------------+-----------------+\n",
      "+---------------------------+--------------------+---------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('hadoop_catalog.default.geolocation').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a129f3-5b1f-42d3-8974-cc837dd32b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
